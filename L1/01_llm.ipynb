{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d9cbcf",
   "metadata": {},
   "source": [
    "# **AI Studio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1a1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make decisions or predictions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, how wonderful!\n",
      "\n",
      "Do they get along well, and what are their names?\n",
      "Based on your 2 dogs, there would be **8 paws** in your house (4 paws per dog).\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"YOUR_API_KEY\")\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\")\n",
    "\n",
    "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
    "print(response.text)\n",
    "\n",
    "response = chat.send_message(\"How many paws are in my house?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7f8f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role - user: I have 2 dogs in my house.\n",
      "role - model: Oh, how wonderful!\n",
      "\n",
      "Do they get along well, and what are their names?\n",
      "role - user: How many paws are in my house?\n",
      "role - model: Based on your 2 dogs, there would be **8 paws** in your house (4 paws per dog).\n"
     ]
    }
   ],
   "source": [
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55249eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hey\n",
      "Gemini: Hey there! Got it. No worries at all.\n",
      "\n",
      "So, just to recap the current situation:\n",
      "\n",
      "*   There's Hamdan, who owns 2 dogs and asked about paws.\n",
      "*   And now there's *you*, a new person.\n",
      "\n",
      "How can I help you today? Do you have a question, or would you like to chat about something?\n",
      "You: Lol no\n",
      "Gemini: Okay, \"Lol no\" to my last recap means I'm still getting it wrong, and I apologize for the confusion!\n",
      "\n",
      "Let me try again, and please correct me if I'm off.\n",
      "\n",
      "It seems like:\n",
      "\n",
      "*   You are **Hamdan**.\n",
      "*   You are the one with **2 dogs**.\n",
      "*   And you are the **only person** I'm talking to right now.\n",
      "\n",
      "My mistake was interpreting \"No, I am a new person\" as literally a *second* person joining, rather than you (Hamdan) perhaps wanting to clear the slate or play around with the identity in the chat.\n",
      "\n",
      "So, **Hamdan**, my apologies for the mix-up! Thanks for bearing with me.\n",
      "\n",
      "What would you like to talk about or do now, Hamdan?\n",
      "You: exit\n",
      "Exiting chat.\n"
     ]
    }
   ],
   "source": [
    "# Chatting\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print((\"You: \" + user_input))\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting chat.\")\n",
    "        break\n",
    "    response = chat.send_message(user_input)\n",
    "    print(f\"Gemini: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65f673",
   "metadata": {},
   "source": [
    "# **Groq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25421bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's world of artificial intelligence (AI) and natural language processing (NLP). Here are some reasons why they're important:\n",
      "\n",
      "1. **Efficient Processing**: Fast language models can process large amounts of text data quickly, making them ideal for applications that require rapid response times, such as chatbots, virtual assistants, and language translation software.\n",
      "2. **Improved User Experience**: With fast language models, users can interact with AI systems in a more natural and seamless way. For example, fast language models can generate human-like responses to user queries, enabling more engaging and human-like conversations.\n",
      "3. **Real-Time Applications**: Fast language models enable real-time applications, such as sentiment analysis, entity recognition, and language translation. This is particularly important in applications like customer service, where timely responses are critical.\n",
      "4. **Scalability**: Fast language models can handle large volumes of data and scale to meet the demands of big data and high-performance computing applications. This makes them ideal for applications like text classification, clustering, and topic modeling.\n",
      "5. **Reduced Latency**: Fast language models reduce latency, which is critical in applications like voice assistants, where users expect immediate responses. Reduced latency also improves the overall user experience, as users don't have to wait for the system to respond.\n",
      "6. **Resource Efficiency**: Fast language models can run on smaller devices, such as smartphones or embedded systems, without requiring significant computational resources. This makes them more energy-efficient and cost-effective.\n",
      "7. **Competitiveness**: In today's fast-paced business environment, fast language models can give organizations a competitive edge. For example, a company with a fast language model can respond to customer queries more quickly, improving customer satisfaction and loyalty.\n",
      "8. **Accessibility**: Fast language models can improve accessibility for people with disabilities, such as those with visual or auditory impairments. For example, fast language models can generate text-to-speech or speech-to-text outputs, enabling more accessible communication.\n",
      "9. **Scientific Research**: Fast language models can accelerate scientific research in areas like NLP, machine learning, and cognitive science. By enabling faster experimentation and prototyping, fast language models can help researchers explore new ideas and hypotheses more quickly.\n",
      "10. **Innovation**: Fast language models can enable new innovations and applications, such as language-based interfaces, conversational AI, and human-computer interaction. By providing a foundation for these innovations, fast language models can drive technological progress and advancement.\n",
      "\n",
      "To achieve fast language models, researchers and developers use various techniques, such as:\n",
      "\n",
      "1. **Model pruning**: Reducing the size and complexity of language models to improve computational efficiency.\n",
      "2. **Quantization**: Representing model weights and activations using lower-precision data types to reduce computational requirements.\n",
      "3. **Knowledge distillation**: Transferring knowledge from large, pre-trained models to smaller, more efficient models.\n",
      "4. **Parallelization**: Distributing model computations across multiple processing units or devices to improve processing speed.\n",
      "5. **Specialized hardware**: Using custom-designed hardware, such as GPUs or TPUs, to accelerate language model computations.\n",
      "\n",
      "By leveraging these techniques, fast language models can unlock new applications, improve user experiences, and drive innovation in NLP and AI.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
